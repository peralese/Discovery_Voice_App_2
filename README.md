
# ğŸ™ï¸ Voice Interview Bot â€“ Dynamic Q&A Proof of Concept

This project is a proof of concept (PoC) for a **voice-enabled AI interview assistant** designed to gather cloud discovery information from users through dynamic conversation. It uses speech recognition, Whisper transcription, GPT-powered question generation, and structured context management.

---

## ğŸ”§ Features

- Ask interview questions via **text-to-speech (TTS)** using `pyttsx3`
- Capture answers using **microphone input** via browser or terminal
- **Transcribe voice responses using OpenAI Whisper API**
- Store both **audio (.wav/.webm)** and **transcribed text**
- Dynamically generate questions using **GPT-3.5 / GPT-4**
- Follow-up with clarification questions when needed
- Save structured context to `interview_log.json` and `discovery_output.json`
- **Graceful exit** (user can say "exit" or "cancel")
- Optional **deferred transcription mode**
- ğŸŒ **Web interface using Flask + Web Audio API** (in progress)

---

## ğŸ—‚ï¸ Project Structure

```
Discovery_Voice_App_2/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ context_manager.py          # Schema + context tracking
â”‚   â”œâ”€â”€ question_generator.py       # GPT-powered Q&A generation
â”‚   â”œâ”€â”€ storage.py                  # Save text + audio paths
â”‚   â”œâ”€â”€ transcribe.py               # Whisper integration (live + file-based)
â”‚   â”œâ”€â”€ transcribe_all_responses.py# Batch transcribe (optional)
â”‚   â””â”€â”€ tts.py                      # Text-to-speech playback
â”œâ”€â”€ responses/                      # Saved audio responses (excluded from git)
â”œâ”€â”€ web/                            # Flask app (MVP UI)
â”‚   â”œâ”€â”€ app.py                      # Flask backend
â”‚   â”œâ”€â”€ templates/index.html        # Interview frontend
â”‚   â””â”€â”€ static/script.js            # Audio recording + upload
â”œâ”€â”€ interview_log.json              # Structured interview results
â”œâ”€â”€ discovery_output.json           # Final schema context
â”œâ”€â”€ .env                            # OpenAI API key (excluded from git)
â”œâ”€â”€ requirements.txt                # Dependencies
â””â”€â”€ main.py                         # Main CLI-driven interview script
```

---

## ğŸš€ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/peralese/Discovery_Voice_App_2.git
cd Discovery_Voice_App_2
```

### 2. Create a virtual environment

```bash
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set your OpenAI API key

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY=your_openai_api_key_here
```

---

## â–¶ï¸ Running the App

### Option A: Terminal-Driven Interview

```bash
python main.py
```

The CLI assistant will:
1. Ask questions using TTS
2. Record microphone input
3. Save response audio
4. Transcribe with Whisper
5. Store results and repeat

---

### Option B: Web Interface (Flask MVP)

```bash
cd web
python app.py
```

Then open: [http://localhost:5000](http://localhost:5000)

You can:
- Hear the current question
- Record and upload audio from your browser
- View live Whisper transcription

---

## ğŸ“¦ Roadmap

- [x] Voice-based Q&A loop (terminal)
- [x] Whisper-based transcription (CLI + web)
- [x] GPT-driven question generation
- [x] Exit intent detection
- [x] Deferred transcription mode
- [x] Graceful shutdown support
- [x] Web interface MVP (Flask + JS)
- [ ] Dynamic follow-up improvements (soft fix complete)
- [ ] Export summary to CSV / clean JSON
- [ ] Spoken introduction at interview start
- [ ] Pause/resume support
- [ ] End-of-interview summary report
- [ ] Cloud deployment (e.g., Render, EC2, etc.)

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¤ Author

**Erick Perales** â€” *IT Architect, Cloud Migration Specialist*
